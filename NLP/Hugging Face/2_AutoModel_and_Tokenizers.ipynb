{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lQHAgBt_mw-7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## inside the pipeline"
      ],
      "metadata": {
        "id": "lQHAgBt_mw-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o2Kykdu_5MKd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to numbers\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "tokenized = tokenizer(['He will buy','he will not buy'],padding=True,truncation=True,return_tensors='pt')\n",
        "tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KojqR3Ib5es6",
        "outputId": "bb885e27-25c9-4bf2-fd9f-fb746f246384"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 2002, 2097, 4965,  102,    0],\n",
              "        [ 101, 2002, 2097, 2025, 4965,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Last Hidden State\n",
        "\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "logists = model(tokenized['input_ids'],tokenized['attention_mask']).last_hidden_state\n",
        "logists.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OssoR3zK8REP",
        "outputId": "9843efb7-a99a-4f10-a4e9-725b6bd0b6fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('model')\n",
        "model2 = AutoModel.from_pretrained('model')"
      ],
      "metadata": {
        "id": "c4m63rMEkSJR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Head\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "logits = model(tokenized['input_ids'],tokenized['attention_mask']).logits\n",
        "logits,logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9f19rCC9TEZ",
        "outputId": "feb501b5-5234-4a90-d5b0-39be0791bb70"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-3.5745,  3.7256],\n",
              "         [ 4.1175, -3.3430]], grad_fn=<AddmmBackward0>),\n",
              " torch.Size([2, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scratch models with Config"
      ],
      "metadata": {
        "id": "3VtzV4tqmtTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertModel, AutoTokenizer\n",
        "\n",
        "conf = BertConfig()\n",
        "model = BertModel(conf)\n",
        "model = BertModel.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "IQq0JX_uVTRE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RZzeS9jqHmX",
        "outputId": "bcce4b8f-355a-46c5-ad77-7ba9447ff403"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28996"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "token = tokenizer('France is [MASK]',return_tensors='pt')\n",
        "model(token['input_ids']).last_hidden_state[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHJ0BmUqoF0h",
        "outputId": "a0737f0b-46c2-4ae9-f72f-daf9db80ba1b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3368,  0.0216,  0.0380,  ..., -0.1186,  0.5137,  0.0122],\n",
              "        [ 0.4598, -0.2675,  0.4053,  ..., -0.3439,  0.5116,  0.2068],\n",
              "        [ 0.1169, -0.5137,  0.3464,  ...,  0.0287,  0.2505,  0.3026],\n",
              "        [ 0.1842, -0.4429,  0.1636,  ...,  0.1074,  0.3006, -0.2138],\n",
              "        [ 0.6525, -0.3284, -0.8536,  ..., -0.2074,  1.5712, -0.2596]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hello Hamza'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "tokens = tokenizer.decode(ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb0H_MO7vBCj",
        "outputId": "fa5a3a1b-c13c-45a5-f42f-2f3dae235e0a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Ham', '##za']\n",
            "[8667, 13030, 3293]\n",
            "Hello Hamza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Hello Ali', 'He is going to school']\n",
        "text1 = 'Hello Ali'\n",
        "text2  = 'He is going to school'\n",
        "tokens = tokenizer(text,padding=True)\n",
        "token1 = tokenizer(text1)\n",
        "token2 = tokenizer(text2)"
      ],
      "metadata": {
        "id": "7AG4PnuGyP-4"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-8c18gOz0O2",
        "outputId": "012f8106-d5a7-45cd-e507-813b977cc293"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.tensor(token1['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lc0FmT70Qiu",
        "outputId": "e7444785-befc-4907-aa87-049ab8319859"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 101, 8667, 4149,  102])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor([token1['input_ids']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeaRSVo80JPU",
        "outputId": "3b53dfab-3932-4fa7-b2fa-d97c774365bf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[0.7100, 0.1312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor([token2['input_ids']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVfnR_s40jTP",
        "outputId": "a0da0751-9fbf-4919-a383-c9252dba29fe"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[0.7300, 0.2339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(tokens['input_ids']),attention_mask=torch.tensor(tokens['attention_mask']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK_6ytStyadM",
        "outputId": "542ed49a-9727-464c-bd87-9a4cfe7aa9ff"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[0.7100, 0.1312],\n",
              "        [0.7300, 0.2339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Parameters pass to tokenizer"
      ],
      "metadata": {
        "id": "u-KbHJMFIeWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ['Hello hello','his name is Ali']\n",
        "\n",
        "\n",
        "# Padding\n",
        "\n",
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "\n",
        "\n",
        "\n",
        "# Truncation\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ],
      "metadata": {
        "id": "5Vg9J1BDy0tj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}